{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "dfde = gpd.read_parquet('../data/processed_datasets/simplified_refined_DFDE_1984_2021_EPSG2154_FR.parquet') #ok \n",
    "hm = gpd.read_parquet('../data/processed_datasets/simplified_health-monitoring_2007-2023_EPSG2154_FR.parquet') #ok \n",
    "nfi = gpd.read_parquet('../data/processed_datasets/simplified_NFI_2003-2021_EPSG2154_FR.parquet') #ok \n",
    "senfseidl = gpd.read_parquet(\"../data/processed_datasets/simplified_SenfSeidl_joined_EPSG2154_FR.parquet\") #ok \n",
    "bdiff = gpd.read_parquet('../data/processed_datasets/simplified_merge_lilan_bdiff_2012_2022_FR_EPSG2154.parquet') #ok \n",
    "cdi = gpd.read_parquet('../data/processed_datasets/simplified_CDI_2012_2023_EPSG2154_FR.parquet') #ajouter tree-specie\n",
    "forms = gpd.read_parquet('../data/processed_datasets/simplified_FORMS_clearcut_2017_2020_EPSG2154.parquet') #ok\n",
    "\n",
    "\n",
    "#preprocessing bdiff:\n",
    "bdiff['class'] = 'Fire'\n",
    "bdiff = bdiff[ bdiff.forest_area_m2 > 5000 ]\n",
    "#create function that convert column 'start_date' to pd.datetime with the following format : '%Y-%m-%d', \n",
    "# if the 'start_date' is just mentionning the time and not the date, use the year column to create a start_date and end_date column as the first and last day of this year\n",
    "#create this function as 'to_datetime_safe', it takes as input a row of a dataframe\n",
    "def to_datetime_safe(row):\n",
    "    try :\n",
    "        return pd.to_datetime(row['start_date'], format='%Y-%m-%d'), pd.to_datetime(row['start_date'], format='%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return pd.to_datetime(row['year'], format='%Y'), pd.to_datetime(row['year'], format='%Y') + pd.offsets.YearEnd(0)\n",
    "\n",
    "\n",
    "bdiff[['start_date', 'end_date']] = bdiff.apply(to_datetime_safe, axis=1, result_type='expand')\n",
    "\n",
    "#preprocessing tcl:\n",
    "tcl = gpd.read_parquet('../data/processed_datasets/simplified_TreeCoverLoss_2001-2022_EPSG2154_FR.parquet')\n",
    "tcl['year'] = tcl['year'] + 2000\n",
    "tcl['class'] = None\n",
    "\n",
    "#autoreload \n",
    "from attribution2 import Attribution\n",
    "from constants import DCLASS_SCORE\n",
    "\n",
    "temporal_buffer = 2 \n",
    "\n",
    "#subset of datasets\n",
    "ddataset = {'dfde': dfde, 'hm': hm, 'nfi': nfi, 'senfseidl': senfseidl, 'bdiff': bdiff, 'cdi':cdi, 'forms':forms}\n",
    "dtypes = {'dfde': 'polygon', 'hm': 'point', 'nfi': 'point', 'senfseidl': 'point', 'bdiff': 'polygon', 'cdi':'polygon', 'forms':'point'}\n",
    "\n",
    "ddataset_profile = {\n",
    "    'dfde': {\n",
    "        'spatial': ('offset_gaussian', {'offset': 150, 'decrease': 5000}), #offset srt( min(area) / pi), k sqrt(median(area) / pi)\n",
    "        'temporal': ('step', {'start': 0, 'end': 365})\n",
    "    },\n",
    "    'hm': {\n",
    "        'spatial': ('offset_gaussian', {'offset': 10, 'decrease': 100}),\n",
    "        'temporal': ('step', {'start': 0, 'end': 365})\n",
    "    },\n",
    "    'nfi': {\n",
    "        'spatial': ('offset_gaussian', {'offset': 600, 'decrease': 25}),\n",
    "        'temporal': ('step', {'start': 0, 'end': 5 * 365})\n",
    "    },\n",
    "    'bdiff': {\n",
    "        'spatial': ('weighting_function', {'x0': 500, 'k': 500}),\n",
    "        'temporal': ('gaussian', {'mean': 0, 'std': 30})\n",
    "    },\n",
    "    'senfseidl': {\n",
    "        'spatial': ('offset_gaussian', {'offset': 30, 'decrease': 5 * 30}),\n",
    "        'temporal': ('offset_gaussian', {'offset': 1 * 365, 'decrease': 1.5 * 365})\n",
    "    },\n",
    "    'forms': {\n",
    "        'spatial': ('offset_gaussian', {'offset': 10, 'decrease': 5*10}),\n",
    "        'temporal': ('step', {'start': 0, 'end': 365})\n",
    "    },\n",
    "    'reference': {\n",
    "        'spatial': ('offset_gaussian', {'offset': 0, 'decrease': 3 * 50}),\n",
    "        'temporal': ('step', {'start': 0, 'end': 365})\n",
    "    },\n",
    "    'cdi': {\n",
    "        'spatial': ('offset_gaussian', {'offset': 5000, 'decrease': 500}),\n",
    "        'temporal': ('offset_gaussian', {'offset': 1 * 365, 'decrease': 365})\n",
    "    },\n",
    "}\n",
    "\n",
    "#visible disturbances\n",
    "ddisturbance_profile = {\n",
    "    'fire': {\n",
    "        'spatial': ('gaussian', {'mean': 0, 'std': 500}),\n",
    "        'temporal': ('gaussian', {'mean': 0, 'std': 3*365})\n",
    "    },\n",
    "    'storm': {\n",
    "        'spatial': ('gaussian', {'mean': 0, 'std': 2000}),\n",
    "        'temporal': ('gaussian', {'mean': 0, 'std': 1.5 * 365})\n",
    "    },\n",
    "    'biotic-dieback': {\n",
    "        'spatial': ('gaussian', {'mean': 0, 'std': 1000}),\n",
    "        'temporal': ('gaussian', {'mean': 0, 'std': 365})\n",
    "    },\n",
    "    'drought-dieback': {\n",
    "        'spatial': ('gaussian', {'mean': 0, 'std': 2500}),\n",
    "        'temporal': ('gaussian', {'mean': 0, 'std': 2*365})\n",
    "    },\n",
    "    'biotic-mortality': {\n",
    "        'spatial': ('gaussian', {'mean': 0, 'std': 250}),\n",
    "        'temporal': ('gaussian', {'mean': 0, 'std': 3*365})\n",
    "    },\n",
    "    'anthropogenic': {\n",
    "        'spatial': ('gaussian', {'mean': 0, 'std': 500}),\n",
    "        'temporal': ('gaussian', {'mean': 0, 'std': 1 * 365})\n",
    "    }\n",
    "}\n",
    "\n",
    "doa = {'dfde': 1.0, 'hm': 1.0, 'nfi': 1.0, 'senfseidl': .75, 'bdiff': 1.0, 'cdi':.75, 'forms':0.75}\n",
    "dsbuffer = {'dfde': None, 'hm': 5000, 'nfi': 7000, 'senfseidl': 100, 'bdiff': None, 'cdi':100, 'forms':100}\n",
    "\n",
    "attribution = Attribution(ddataset, reference=tcl, doa=doa, dtypes=dtypes, \n",
    "                          temporal_buffer=temporal_buffer, dsbuffer=dsbuffer, \n",
    "                          dclass_score=DCLASS_SCORE, granularity=5, \n",
    "                          ddataset_profile=ddataset_profile, ddisturbance_profile=ddisturbance_profile, \n",
    "                          start_year=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get louvain communities\n",
    "from utils import get_temporal_range\n",
    "from shapely import box\n",
    "from shapely.geometry import shape \n",
    "import numpy as np\n",
    "bounds_4326 = (4.75,47.10,6.82,48.51)\n",
    "pol_4326 = box(*bounds_4326)\n",
    "import rasterio \n",
    "from rasterio.warp import transform_geom\n",
    "pol_2154 = shape(transform_geom('epsg:4326', attribution.dataset.crs, pol_4326))\n",
    "\n",
    "_dataset = attribution.dataset.clip(pol_2154)\n",
    "n = _dataset.shape[0] / attribution.dataset.shape[0] * 100\n",
    "_spatial_entity_dataset = attribution.spatial_entity_dataset.clip(pol_2154)\n",
    "m = _spatial_entity_dataset.shape[0] / attribution.spatial_entity_dataset.shape[0] * 100\n",
    "temporal_threshold = 180 * 2\n",
    "spatial_threshold = 600\n",
    "resolution = 100\n",
    "print(n, m)\n",
    "import os \n",
    "from tqdm import tqdm \n",
    "import networkx as nx \n",
    "b = \"_\".join([str(x) for x in bounds_4326])\n",
    "\n",
    "temporal_threshold_ = temporal_threshold \n",
    "spatial_threshold_ = spatial_threshold\n",
    "if os.path.isfile(f\"../data/results/clusters/communities_r{resolution}_g{attribution.granularity}_{spatial_threshold_}m_{temporal_threshold_}d_v{attribution.version}_b{b}_n{n :.0f}_m{m :.0f}.parquet\"):\n",
    "    all_clusters_gdf = gpd.read_parquet(f\"../data/results/clusters/communities_r{resolution}_g{attribution.granularity}_{spatial_threshold_}m_{temporal_threshold_}d_v{attribution.version}_b{b}_n{n :.0f}_m{m :.0f}.parquet\")\n",
    "    c = all_clusters_gdf.shape[0] / (_spatial_entity_dataset.shape[0] + _dataset.shape[0] )\n",
    "    dataset = _dataset[['geometry', 'dataset', 'class', 'centroid_date']]\n",
    "    spatial_entity_dataset = _spatial_entity_dataset[['geometry', 'centroid_date']]\n",
    "else : \n",
    "    dataset = _dataset[['geometry', 'dataset', 'class', 'centroid_date']]\n",
    "    spatial_entity_dataset = _spatial_entity_dataset[['geometry', 'centroid_date']]\n",
    "    sindex = dataset.sindex\n",
    "    spatial_entity_sindex = spatial_entity_dataset.sindex\n",
    "\n",
    "    if os.path.isfile(f\"../data/results/graph/graph_g{attribution.granularity}_{spatial_threshold_}m_{temporal_threshold_}d_v{attribution.version}_b{b}_n{n :.0f}_m{m :.0f}.gml\"):\n",
    "        G = nx.read_gml(f\"../data/results/graph/graph_g{attribution.granularity}_{spatial_threshold_}m_{temporal_threshold_}d_v{attribution.version}_b{b}_n{n :.0f}_m{m :.0f}.gml\")\n",
    "\n",
    "    else: \n",
    "        # Assuming gdf is your GeoDataFrame\n",
    "        print('Building graph...')\n",
    "        G = nx.Graph()\n",
    "        dataset_loc = dataset\n",
    "        #while graph not connected build graph by doubling thresholds\n",
    "        N = 3\n",
    "        i = 0\n",
    "        while (len(G) == 0 or not nx.is_connected(G)) and (i < N or len(dataset_loc) > 0):\n",
    "            G = build_graph(dataset_loc, dataset, sindex, spatial_threshold, temporal_threshold, attribution, G=G)\n",
    "            spatial_threshold *= 2\n",
    "            temporal_threshold *= 2\n",
    "            print(f'graph not connected, new thresholds : {spatial_threshold}m, {temporal_threshold}d')\n",
    "            #set d as the dataset with the events not in the graph\n",
    "            dataset_loc = dataset[~dataset.index.isin(G.nodes())]\n",
    "            i += 1\n",
    "        \n",
    "        nx.write_gml(G, f\"../data/results/graph/graph_{attribution.granularity}_{spatial_threshold_}_{temporal_threshold_}_{attribution.version}_b{b}_n{n :.0f}_m{m :.0f}.gml\")\n",
    "\n",
    "    # islands = list(nx.connected_components(G))\n",
    "    communities = nx.community.louvain_communities(G, seed=0, resolution=resolution)\n",
    "\n",
    "    # Create a list to store the sub-GeoDataFrames\n",
    "    island_gdfs = []\n",
    "\n",
    "    print('Building islands...')\n",
    "    # Iterate over each island and create a sub-GeoDataFrame\n",
    "    for island in tqdm(communities):\n",
    "        # Select rows from the original GeoDataFrame that correspond to the current island\n",
    "        island_gdf = attribution.dataset.iloc[list(island)]\n",
    "        \n",
    "        # Append this sub-GeoDataFrame to the list\n",
    "        island_gdfs.append(island_gdf)\n",
    "\n",
    "    for i, island_gdf in tqdm(enumerate(island_gdfs)):\n",
    "        # Calculate the envelope (bounding box) of the cluster\n",
    "        envelope = island_gdf.unary_union.envelope\n",
    "\n",
    "        # Calculate the temporal range of the cluster\n",
    "        cluster_start, cluster_end = get_temporal_range(island_gdf)\n",
    "\n",
    "        # Find potential matches using spatial index\n",
    "        possible_matches_index = list(spatial_entity_sindex.intersection(envelope.bounds))\n",
    "        possible_matches = spatial_entity_dataset.iloc[possible_matches_index]\n",
    "\n",
    "        if len(possible_matches_index) > 0 :\n",
    "            break\n",
    "        # Initialize an empty list to store events to be added\n",
    "        events_to_add = []\n",
    "\n",
    "        # Iterate through each potential match\n",
    "        for event in possible_matches.itertuples(index=True):\n",
    "            # Check spatial intersection\n",
    "            spatial_condition = envelope.intersects(event.geometry)\n",
    "            # Check temporal intersection\n",
    "            event_start = event.start_date  # Replace with your actual column name\n",
    "            event_end = event.end_date\n",
    "            temporal_condition = ((event_start <= cluster_end + temporal_threshold) and (event_end >= cluster_end - temporal_threshold)) or ((event_start <= cluster_start + temporal_threshold) and (event_end >= cluster_start - temporal_threshold)) \n",
    "\n",
    "            # If both conditions are met, add the event to the list\n",
    "            if spatial_condition and temporal_condition:\n",
    "                events_to_add.append(event.Index)\n",
    "\n",
    "        # Add the events to the cluster GeoDataFrame\n",
    "        if len(events_to_add) > 0:\n",
    "            additional_events = attribution.spatial_entity_dataset.loc[events_to_add]\n",
    "            island_gdfs[i] = gpd.GeoDataFrame(pd.concat([island_gdf, additional_events]), geometry='geometry', crs=island_gdf.crs)\n",
    "    \n",
    "    # Initialize an empty list to store the modified cluster GeoDataFrames          \n",
    "    modified_gdfs = []\n",
    "\n",
    "    # Add a 'cluster' column and concatenate\n",
    "    for i, island_gdf in tqdm(enumerate(island_gdfs)):\n",
    "        island_gdf['cluster'] = i  # Add a 'cluster' column with the cluster index\n",
    "        modified_gdfs.append(island_gdf)\n",
    "\n",
    "\n",
    "    # Ensure the GeoDataFrame has the correct geometry set\n",
    "    all_clusters_gdf = gpd.GeoDataFrame(pd.concat(modified_gdfs), geometry='geometry').drop(columns=['year'])\n",
    "\n",
    "    # Save to GeoParquet\n",
    "    c = all_clusters_gdf.shape[0] / (_spatial_entity_dataset.shape[0] + _dataset.shape[0] )\n",
    "    all_clusters_gdf.to_parquet(f\"../data/results/clusters/communities_r{resolution}_g{attribution.granularity}_{spatial_threshold}m_{temporal_threshold}d_v{attribution.version}_b{b}_n{n :.0f}_m{m :.0f}.parquet\")\n",
    "\n",
    "print(f'conversion rate : {c :.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.affinity import translate\n",
    "\n",
    "def translate_polygon(df, polygon_column):\n",
    "    def shift_polygon(polygon):\n",
    "        # Random shifts in meters\n",
    "        x_shift = np.random.normal(0, 100)  # Longitude shift\n",
    "        y_shift = np.random.normal(0, 100)  # Latitude shift\n",
    "        return translate(polygon, xoff=x_shift, yoff=y_shift)\n",
    "\n",
    "    df[polygon_column] = df[polygon_column].apply(shift_polygon)\n",
    "    return df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def translate_time(df, start_date_column, end_date_column):\n",
    "    # Applying a Gaussian disturbance with mean = 0 and std = 90 days\n",
    "    disturbance_start = np.random.normal(0, 90, size=len(df))\n",
    "    disturbance_end = np.random.normal(0, 90, size=len(df))\n",
    "\n",
    "    df[start_date_column] = df[start_date_column] + pd.to_timedelta(disturbance_start, unit='d')\n",
    "    df[end_date_column] = df[end_date_column] + pd.to_timedelta(disturbance_end, unit='d')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define clusters\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, adjusted_mutual_info_score\n",
    "import warnings\n",
    "from joblib import Parallel, delayed \n",
    "import pandas as pd \n",
    "from utils import compute_tree_coherence, compute_class_similarity, get_cluster\n",
    "from constants import DCLASS_SCORE\n",
    "import time \n",
    "\n",
    "disturbances = 10\n",
    "methods = ['DBSCAN', 'SpectralClustering'] \n",
    "dcustom_similarity_function = {'tree specie relatedness': (compute_tree_coherence, {}, 1.0), 'class relatedness': (compute_class_similarity, {'dclass_score': DCLASS_SCORE}, 1.0)}\n",
    "weights = [0.40759976, 0.23017731, 0.20566699, 0.15655594]\n",
    "\n",
    "from collections import defaultdict\n",
    "dict_method_metric = defaultdict(list)\n",
    "\n",
    "for method in methods:\n",
    "    ref = gpd.read_parquet(f\"../data/results/clusters/clusters_m{method}_r{resolution}_g{attribution.granularity}_{spatial_threshold_}m_{temporal_threshold_}d_v{attribution.version}_b{b}_n{n :.0f}_m{m :.0f}.parquet\")\n",
    "    list_reflabels = []\n",
    "    for row in tqdm(ref.itertuples()):\n",
    "        for id_ in row.Indexes:\n",
    "            list_labels.append((id_, row.Index))\n",
    "    df_reflabel = pd.DataFrame(list_labels, columns=['id', 'label']).sort_values(by='id')\n",
    "\n",
    "    for j in range(disturbances):\n",
    "        #disturb time period and polygons \n",
    "        all_clusters_gdf = translate_polygon(all_clusters_gdf, 'geometry')\n",
    "        all_clusters_gdf = all_clusters_gdf.apply(translate_time, axis=1, args=('start_date', 'end_date'))\n",
    "\n",
    "        #grouping \n",
    "        groups = all_clusters_gdf.groupby('cluster')\n",
    "        start = time.time()\n",
    "        with warnings.catch_warnings():\n",
    "            list_gdf = []\n",
    "            list_matrices = []\n",
    "            list_labels = []\n",
    "            warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "            for i in tqdm(range(0, len(groups), 1000)):\n",
    "                r = Parallel(n_jobs=-1, prefer='processes', verbose=0)(delayed(get_cluster)(data=data, dtypes_=attribution.dtypes_, dcustom_similarity_function=dcustom_similarity_function, doa=attribution.doa, dclass_score=DCLASS_SCORE, final_weighting_dict=attribution.final_weighting_dict, weights=weights, method=method) for _,data in islice(groups, i, min(len(groups), i+1000)))\n",
    "                for x in r:\n",
    "                    list_gdf.append(x[0])\n",
    "                    list_matrices.append(x[1][0])\n",
    "                    list_labels.append(x[1][1])\n",
    "\n",
    "        end = time.time()\n",
    "        print(f'elapsed time : {end - start :.2f}s')\n",
    "\n",
    "        df = pd.concat(list_gdf)\n",
    "        gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=attribution.dataset.crs)\n",
    "        \n",
    "        list_labels = []\n",
    "        for row in tqdm(gdf.itertuples()):\n",
    "            for id_ in row.Indexes:\n",
    "                list_labels.append((id_, row.Index))\n",
    "        df_label = pd.DataFrame(list_labels, columns=['id', 'label']).sort_values(by='id')\n",
    "        ARI = adjusted_rand_score(df_reflabel.label, df_label.label)\n",
    "        AMI = adjusted_mutual_info_score(df_reflabel.label, df_label.label)\n",
    "        dict_method_metric[method].append((ARI, AMI))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
